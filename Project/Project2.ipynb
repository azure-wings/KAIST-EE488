{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Project2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms3Go43QRaIU"
      },
      "source": [
        "# **EE488 Machine Learning Basics and Practices**\n",
        "## **Mini-Assignment 2**\n",
        "\n",
        "## **Part 1.** Implement DBSCAN algorithm using Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaeOlfy6tox-"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import make_blobs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXPvSaqRX6XO"
      },
      "source": [
        "## Step 1: Implement DBSCAN from skeleton code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4HTeh1mRD6v"
      },
      "source": [
        "## **TODO** : Fill in the blanks of the codes and write your own description of the source code in your report.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYXW5JSsp1V9"
      },
      "source": [
        "class DBSCAN:\n",
        "\n",
        "    def __init__(self, eps, min_samples):\n",
        "        self.eps = eps\n",
        "        self.min_samples = min_samples\n",
        "\n",
        "    def fit_predict(self, X):\n",
        "        '''\n",
        "        parameter:\n",
        "        X: data\n",
        "        shape of X: (number of sample, feature dimensions)\n",
        "\n",
        "        It returns a array of cluster labels for each sample point. \n",
        "        Here, the cluster labels are numbered starting from 1. \n",
        "        Moreover, \"-1\" means noise.\n",
        "        '''\n",
        "        self.X = X\n",
        "        self.cluster_labels = np.zeros((len(X)))              # For all sample points, initialize the cluster label as 0\n",
        "        self.current_label = 0\n",
        "        self.labeled_indices = set()                          # for tracking all visited (already labeled) points\n",
        "        self.core_sample_indices_ = []                        # for tracking all core samples (just for future visualization)\n",
        "\n",
        "        # declare something if you think you need ------------\n",
        "  \n",
        "\n",
        "        # ----------------------------------------------------\n",
        "\n",
        "        while np.any(self.cluster_labels == 0):               # repeat until there's no point that has not been visited any more\n",
        "            p_idx = self.pick_arbitrary_point()               # pick arbitrary point among points that are not visited so far (fill the blanks for \"pick_arbitrary_point\" below)\n",
        "            if self.is_core_sample(p_idx):                    # check if it is core sample (fill the blanks for \"is_core_sample\" below)\n",
        "                self.current_label += 1                       # define new cluster label for current visiting\n",
        "                self.visit_all_successive_neighbors(p_idx)    # visit all the neighbors and label them in succession (fill the blanks for \"visit_all_successive_neighbors\" below) \n",
        "            else:\n",
        "                self.cluster_labels[p_idx] = -1               # label it as noise if it is not a core sample in the first place\n",
        "                \n",
        "        return self.cluster_labels\n",
        "\n",
        "    def pick_arbitrary_point(self):\n",
        "        '''\n",
        "        Pick arbitrary point among points that are not visited so far (for next successive visiting).\n",
        "        It returns an \"index\" of point(\"p_idx\"), not a data point itself.\n",
        "        '''\n",
        "        # fill in the blank --------------------------------------\n",
        "        \n",
        "        # --------------------------------------------------------\n",
        "        assert self.cluster_labels[p_idx] == 0  # sanity check\n",
        "        return p_idx\n",
        "\n",
        "    def is_core_sample(self, p_idx):\n",
        "        '''\n",
        "        parameter:\n",
        "        p_idx: index of point\n",
        "\n",
        "        Check whether the \"p_idx\" is a core sample or not.\n",
        "        If it is, return True. Otherwise, return False.\n",
        "        You can use \"get_neighbors\" method, which is defined below.\n",
        "        You can define a core sample that has greater than or equal to min_samples points in its neighbor, where the point itself is also included in its neighbor.\n",
        "        '''\n",
        "        # fill in the blank --------------------------------------\n",
        "        \n",
        "        # --------------------------------------------------------\n",
        "\n",
        "    def visit_all_successive_neighbors(self, p_idx):\n",
        "        '''\n",
        "        parameter:\n",
        "        p_idx: index of point\n",
        "\n",
        "        Visit all the neighbors of \"p_idx\" as well as the neighbors of all the visited points if they are the core points themselves.\n",
        "        Assign current cluster label everytime you visited. But you don't need to relabel them if they are already allocated to a specific cluster.\n",
        "        It returns nothing but modifies \"self.cluster_labels\" in-place when labeling.\n",
        "        '''\n",
        "\n",
        "        all_neighbors_indices = {p_idx}\n",
        "\n",
        "        while all_neighbors_indices:\n",
        "\n",
        "        # fill in the blank --------------------------------------\n",
        "        \n",
        "        # --------------------------------------------------------\n",
        "\n",
        "    def get_neighbors(self, p_idx):\n",
        "        '''\n",
        "        parameter:\n",
        "        p_idx: index of point\n",
        "\n",
        "        It returns a \"set of indices\" of neighbors of \"p_idx\" point.\n",
        "        l2 norm will be considered for computing distances.\n",
        "        '''\n",
        "        # fill in the blank --------------------------------------\n",
        "        \n",
        "        # --------------------------------------------------------\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6A2goNGYITM"
      },
      "source": [
        "## Step 2: Clusterting with two different datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyLzKBLrXsFN"
      },
      "source": [
        "## **TODO** : Try three different (eps, MinPts) combinations(depeding on your choice) for each dataset and attach the results in the report. Describe how the results change with varying eps and MinPts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPe2bexh2gbE"
      },
      "source": [
        "noisy_moons = datasets.make_moons(n_samples=1500, noise=.05, random_state=0)\n",
        "X, labels_true = noisy_moons\n",
        "\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Compute DBSCAN\n",
        "# -----------------------------------------\n",
        "db = DBSCAN(eps=0.2, min_samples=4) \n",
        "# -----------------------------------------\n",
        "db.fit_predict(X)\n",
        "core_samples_mask = np.zeros_like(db.cluster_labels, dtype=bool)\n",
        "core_samples_mask[db.core_sample_indices_] = True\n",
        "labels = db.cluster_labels\n",
        "\n",
        "# Number of clusters in labels, ignoring noise if present.\n",
        "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise_ = list(labels).count(-1)\n",
        "\n",
        "print('Estimated number of clusters: %d' % n_clusters_)\n",
        "print('Estimated number of noise points: %d' % n_noise_)\n",
        "\n",
        "# Plot result\n",
        "# Black removed and is used for noise instead.\n",
        "unique_labels = set(labels)\n",
        "colors = [plt.cm.Spectral(each)\n",
        "          for each in np.linspace(0, 1, len(unique_labels))]\n",
        "for k, col in zip(unique_labels, colors):\n",
        "    if k == -1:\n",
        "        # Black used for noise.\n",
        "        col = [0, 0, 0, 1]\n",
        "\n",
        "    class_member_mask = (labels == k)\n",
        "\n",
        "    xy = X[class_member_mask & core_samples_mask]\n",
        "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "             markeredgecolor='k', markersize=14)\n",
        "\n",
        "    xy = X[class_member_mask & ~core_samples_mask]\n",
        "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "             markeredgecolor='k', markersize=6)\n",
        "\n",
        "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNUmOIA73RtX"
      },
      "source": [
        "# Generate sample data\n",
        "centers = [[1, 1], [-1, -1], [1, -1], [-1, 1]]\n",
        "X, labels_true = make_blobs(n_samples=1000, centers=centers, cluster_std=0.4,\n",
        "                            random_state=0)\n",
        "\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Compute DBSCAN\n",
        "# -----------------------------------------\n",
        "db = DBSCAN(eps=0.2, min_samples=6)     ## Try different eps and min_samples values by yourself\n",
        "# -----------------------------------------\n",
        "db.fit_predict(X)\n",
        "core_samples_mask = np.zeros_like(db.cluster_labels, dtype=bool)\n",
        "core_samples_mask[db.core_sample_indices_] = True\n",
        "labels = db.cluster_labels\n",
        "\n",
        "# Number of clusters in labels, ignoring noise if present.\n",
        "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise_ = list(labels).count(-1)\n",
        "\n",
        "print('Estimated number of clusters: %d' % n_clusters_)\n",
        "print('Estimated number of noise points: %d' % n_noise_)\n",
        "\n",
        "# Plot result\n",
        "# Black removed and is used for noise instead.\n",
        "unique_labels = set(labels)\n",
        "colors = [plt.cm.Spectral(each)\n",
        "          for each in np.linspace(0, 1, len(unique_labels))]\n",
        "for k, col in zip(unique_labels, colors):\n",
        "    if k == -1:\n",
        "        # Black used for noise.\n",
        "        col = [0, 0, 0, 1]\n",
        "\n",
        "    class_member_mask = (labels == k)\n",
        "\n",
        "    xy = X[class_member_mask & core_samples_mask]\n",
        "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "             markeredgecolor='k', markersize=14)\n",
        "\n",
        "    xy = X[class_member_mask & ~core_samples_mask]\n",
        "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "             markeredgecolor='k', markersize=6)\n",
        "\n",
        "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPQCYym8X0ut"
      },
      "source": [
        "## **TODO** : Please answer the following questions (cleary write down your own explanation on your answer to get a full credit)\n",
        "\n",
        "### **Q1.** What is the main difference between noise and border point?\n",
        "### **Q2.** Let us slightly modify the algorithm (in `fit_predict` method) as follows:\n",
        "\n",
        "### From\n",
        "```python\n",
        "        while np.any(self.cluster_labels == 0):               # repeat until there's no point that has not been visited any more\n",
        "            p_idx = self.pick_arbitrary_point()               # pick arbitrary point among not-visited points\n",
        "            if self.is_core_sample(p_idx):                    # check if it is core sample\n",
        "                self.current_label += 1                       # define new cluster label for current visiting\n",
        "                self.visit_all_successive_neighbors(p_idx)    # visit all the neighbors and label them in succession\n",
        "            else:\n",
        "                self.cluster_labels[p_idx] = -1               # label it as noise if it is not a core sample in the first place\n",
        "\n",
        "        return self.cluster_labels\n",
        "```\n",
        "### to \n",
        "```python\n",
        "        while np.any(self.cluster_labels == 0):               # repeat until there's no point that has not been visited any more\n",
        "            p_idx = self.pick_arbitrary_point()               # pick arbitrary point among not-visited points\n",
        "            if self.is_core_sample(p_idx):                    # check if it is core sample\n",
        "                self.current_label += 1                       # define new cluster label for current visiting\n",
        "                self.visit_all_successive_neighbors(p_idx)    # visit all the neighbors and label them in succession\n",
        "            else:\n",
        "                self.cluster_labels[p_idx] = -1               # label it as noise if it is not a core sample in the first place\n",
        "                self.labeled_indices.add(p_idx)               # mark it as labeled point (Hence, you don't relabel this point again later)\n",
        "\n",
        "        return self.cluster_labels\n",
        "```\n",
        "### Then, what is the possible range of number of noise samples after the clustering is completed? **(write down in general form and express it using the number of differnt types of points(e.g. core, noise) which are assumed to be obtained in orignal DBSCAN.  The modified version run with the same and fixed eps, MinPts that the original one used)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3hYfe4sL8eq"
      },
      "source": [
        "Visualization Reference: https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzAfKTk75CXJ"
      },
      "source": [
        "\n",
        "# **Part 2.** Implement the Batchnormalization Layer in Deep Neural Network\n",
        "\n",
        "## Objective\n",
        "1. Implementing the batchnormalization layer that can be used like ```nn.BatchNorm1d()``` module\n",
        "2. Understading how to introduce and apply the learnable rescaling and reshifting parameters to the normlized input\n",
        "3. Comparing the difference between the results with and without the batchnormalization layer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzrVHz2x49vE"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import numpy as np \n",
        "import random\n",
        "import time \n",
        "\n",
        "torch.backends.cudnn.deterministic = True # Use cudnn as deterministic mode for reproducibility\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwoT7Pvh9Dk4"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## Step1: Load CIFAR-10\n",
        "- Load **CIFAR-10** dataset using `torchvision` package\n",
        "- For training data, we utilize appropriate **data augmentation** and **data preprocessing**\n",
        "  - We utilize **Random Crop** and **Random Horizontal Filp** for data augmentation\n",
        "  - Data is preprocessed by **normalizing** the data using the mean (0.4914, 0.4822, 0.4465 for each of RGB channel, respectively) and standard deviation (0.2023, 0.1994, 0.2010 for each of RGB channel, respectively) computed from training set of CIFAR-10.\n",
        "- For test data, we utilize only **data preprocessing**, not **data augmentation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCs0Mq3I89QY"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4), # Random Crop: Randomly crop the part of the large image and utilize it as an augmented data \n",
        "        transforms.RandomHorizontalFlip(), # Random Horizontal Flip: Randomly flip the image and utilize it as an augmented data\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023,0.1994,0.2010]), # Normalize the data using the given mean and standard deviation\n",
        "        ])\n",
        "\n",
        "#Apply data preprocessing for test set\n",
        "transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(), \n",
        "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023,0.1994,0.2010]),\n",
        "        ]) \n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# load cifar10 with only 0~3 classes\n",
        "train_indices, = torch.where(torch.tensor(train_dataset.targets) <= 3)\n",
        "test_indices, = torch.where(torch.tensor(test_dataset.targets) <= 3)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, sampler=SubsetRandomSampler(train_indices), batch_size=256) \n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, sampler=SubsetRandomSampler(test_indices), batch_size=256)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs_FEbzv9KT1"
      },
      "source": [
        "- Implement `reset_seed` function for reproducibility\n",
        "  - `reset_seed` function sets the random seed for `torch`, `numpy` and `random` pacakges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt0cm2859Hfx"
      },
      "source": [
        "def reset_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlCbcivx9frI"
      },
      "source": [
        "## Step2: Run DNN(Deep Neural Network) model without Batchnorm.\n",
        "\n",
        "\n",
        "- Run the experiment without batchnorm for the baseline.\n",
        "\n",
        "\n",
        "## **TODO** : Fill in the blanks of the codes in step 2 and write your own description of the source code in your report. (You can refer to the practice session material for a few blanks)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mAlIcCR-0Kb"
      },
      "source": [
        "**Step2-1**: Implement DNN model **with six fully-connected layers(i.e. five hidden layers)**\n",
        "\n",
        "### **In ```forward()``` method, all fully-connected layers must be followed by ReLU activation except for the last layer** (Use ```F.relu``` function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVmWkzVV9Lxa"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(DNN, self).__init__()\n",
        "        # torch.nn.Linear(size of each input sample, size of each output sample)\n",
        "        self.fc1 = nn.Linear(3072, 1800) # Input size should be 3072 (32*32*3) and output size is the size of first hidden layer(your design parameter).\n",
        "        self.fc2 = nn.Linear(1800, 1200) # Input size should be the size of first hidden layer and output size is the size of second hidden layer(your design parameter).\n",
        "        self.fc3 = nn.Linear(1200, 1200)\n",
        "        self.fc4 = nn.Linear(1200, 640)\n",
        "        self.fc5 = nn.Linear(640, 320) \n",
        "        self.fc6 = nn.Linear(320, 4) # Input size should be the size of the previous layer and output size should be 4 (the number of classes).\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # fill in the blank --------------------------------------\n",
        "        \n",
        "        # --------------------------------------------------------\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbJaTvbu-2i4"
      },
      "source": [
        "**Step2-2**: Implement `train` function for training (see step 2-4 below to see how we are going to use train function) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlXwZeZj-rlt"
      },
      "source": [
        "def train(model, data_loader, criterion, optimizer, n_epoch):\n",
        "    model.train()\n",
        "    for epoch in range(n_epoch):\n",
        "        running_loss = 0\n",
        "        for i, (images, labels) in enumerate(data_loader):\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "            # fill in the blank --------------------------------------\n",
        "            \n",
        "            # --------------------------------------------------------\n",
        "            \n",
        "        print('Epoch {}, loss = {:.3f}'.format(epoch + 1, running_loss/len(data_loader)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whpG67Pl--DM"
      },
      "source": [
        "**Step2-3**: Implement `eval` function for evaluation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnn-1_MJ-8xv"
      },
      "source": [
        "def eval(model, data_loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "            # fill in the blank --------------------------------------\n",
        "            \n",
        "            # --------------------------------------------------------\n",
        "        accuracy = 100 * correct / total\n",
        "        \n",
        "    print('Test Accuracy: {}%'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gZVZ9Uh_AyM"
      },
      "source": [
        "**Step2-4**: Train the defined DNN model using `train` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELJZCCtq-_FH"
      },
      "source": [
        "reset_seed(2021)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "dnn_model = DNN().to(\"cuda\")\n",
        "optimizer = optim.Adam(params=dnn_model.parameters())\n",
        "\n",
        "train(dnn_model, train_loader, criterion, optimizer, n_epoch=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbtL0V91_UTZ"
      },
      "source": [
        "**Step2-5**: Check the result (We will compare it with later result)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxllDFC0_Dej"
      },
      "source": [
        "eval(dnn_model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9AUvWeilZWV"
      },
      "source": [
        "## Step 3: Implement MyBacthNorm1d() class inheriting nn.Module\n",
        "\n",
        "When you implement the batchnorm layer in this project, you can inherit the ```nn.Module``` to leave the complicated backward calculations to autograd in pytorch and focus only on implementing the pseudocode of the algorithm in lecture note. In other words, you only need to implement the forward pass of batchnorm layer here. The followings are required to do:\n",
        "\n",
        "- Consider the moving averages of minibatch mean and variance for inference time handling\n",
        "\n",
        "- Compute moving averages using  **exponential moving average*** \n",
        "\n",
        "- Consider the case of training and inference seperately (using ```self.training``` attribute in ```nn.Module```)\n",
        "\n",
        "- $\\alpha$ for exponential moving averaging should set to be 0.1 (see wikipedia below)\n",
        "\n",
        "- Implement it so that it can be used like:\n",
        "\n",
        "```python\n",
        "self.norm_layer1 = MyBatchNorm1d(output_num_neuron)\n",
        "```\n",
        "\n",
        "- **Initialize $\\gamma$ as 1's**\n",
        "\n",
        "- **Initialize $\\beta$ as 0's**\n",
        "\n",
        "*see https://en.wikipedia.org/wiki/Moving_average\n",
        "\n",
        "## **TODO** : Fill in the blanks of the codes in step 3 and write your own description of the source code in your report. (You can refer to the practice session material for a few blanks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkU8-lq4la9Z"
      },
      "source": [
        "class MyBatchNorm1d(nn.Module):\n",
        "    def __init__(self, num_features, alpha = 0.1):\n",
        "        super(MyBatchNorm1d, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(num_features)) # register the tensor as a parameter in this module (be treated like module's parameter -> can be learned via optimizer altogether)\n",
        "        self.beta = nn.Parameter(torch.zeros(num_features)) \n",
        "        \n",
        "        # fill in the blank --------------------------------------\n",
        "        \n",
        "        # --------------------------------------------------------\n",
        "        \n",
        "    def forward(self, input):\n",
        "        \n",
        "        # fill in the blank --------------------------------------\n",
        "        \n",
        "        # --------------------------------------------------------\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDtmWCOxl4Vm"
      },
      "source": [
        "## Step4: Implement DNN with MyBatchNorm1d()\n",
        "We will add MyBatchNorm1d() layer that you made above to DNN model.\n",
        "### **Apply BatchNormalization between fully-connected layer and activation.**\n",
        "- DNN have five hidden fully-connected layers, so you need five batchnorm layers.\n",
        "\n",
        "## **TODO1** : Fill in the blanks of the codes in step 4 and write your own description of the source code in your report. (You can refer to the practice session material for a few blanks)\n",
        "\n",
        "## **TODO2** : Attach and compare the results of step 2-5 and step 4-3 in your report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7Cikrf2mCiI"
      },
      "source": [
        "**Step4-1**: Implement `DNN_BatchNorm`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbhQ8Hnnl2DV"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DNN_BatchNorm(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(DNN_BatchNorm, self).__init__()\n",
        "        # torch.nn.Linear(size of each input sample, size of each output sample)\n",
        "        self.fc1 = nn.Linear(3072, 1800) # Input size should be 3072 (32*32*3) and output size is the size of first hidden layer(your design parameter).\n",
        "        self.norm1 = MyBatchNorm1d(1800)\n",
        "        self.fc2 = nn.Linear(1800, 1200) # Input size should be the size of first hidden layer and output size is the size of second hidden layer(your design parameter).\n",
        "        self.norm2 = MyBatchNorm1d(1200)\n",
        "        self.fc3 = nn.Linear(1200, 1200)\n",
        "        self.norm3 = MyBatchNorm1d(1200)\n",
        "        self.fc4 = nn.Linear(1200, 640)\n",
        "        self.norm4 = MyBatchNorm1d(640)\n",
        "        self.fc5 = nn.Linear(640, 320) \n",
        "        self.norm5 = MyBatchNorm1d(320)\n",
        "        self.fc6 = nn.Linear(320, 4) # Input size should be the size of the previous layer and output size should be 4 (the number of classes).\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # fill in the blank --------------------------------------\n",
        "        \n",
        "        # --------------------------------------------------------\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrkV2xolmgH9"
      },
      "source": [
        "**Step4-2**: Train the defined `DNN_BatchNorm` model using `train` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzsxaGoqmciG"
      },
      "source": [
        "reset_seed(2021)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "dnn_batchnorm_model = DNN_BatchNorm().to(\"cuda\")\n",
        "optimizer = optim.Adam(params=dnn_batchnorm_model.parameters())\n",
        "\n",
        "train(dnn_batchnorm_model, train_loader, criterion, optimizer, n_epoch=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h7zXobDmyVt"
      },
      "source": [
        "**Step4-3**: Check the result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk-uCyjwmuP9"
      },
      "source": [
        "eval(dnn_batchnorm_model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTr4p_s1m0yC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}